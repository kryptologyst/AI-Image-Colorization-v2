# AI Image Colorization

A modern, production-ready image colorization application using state-of-the-art deep learning models. This project automatically adds realistic colors to grayscale images using Hugging Face transformers and provides both web and command-line interfaces.

## Features

- **Modern Architecture**: Built with type hints, comprehensive error handling, and logging
- **Multiple Interfaces**: Web UI (Streamlit) and CLI for different use cases
- **Flexible Models**: Support for various Hugging Face colorization models
- **Batch Processing**: Process multiple images at once via CLI
- **Configuration Management**: YAML-based configuration with environment variable overrides
- **Sample Data**: Automatic generation of test images
- **GPU Acceleration**: Automatic GPU detection and utilization
- **Comprehensive Logging**: Detailed logging for debugging and monitoring

## Quick Start

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/kryptologyst/AI-Image-Colorization-v2.git
   cd AI-Image-Colorization-v2
   ```

2. **Create a virtual environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

### Web Interface

Launch the Streamlit web application:

```bash
streamlit run web_app/app.py
```

Then open your browser to `http://localhost:8501` and upload an image to colorize!

### Command Line Interface

#### Colorize a single image:
```bash
python src/cli.py colorize input.jpg output.jpg
```

#### Batch process a directory:
```bash
python src/cli.py batch input_dir/ output_dir/
```

#### Create sample images:
```bash
python src/cli.py samples data/samples/
```

## üìÅ Project Structure

```
0230_Image_colorization/
‚îú‚îÄ‚îÄ src/                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ image_colorizer.py  # Core colorization logic
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration management
‚îÇ   ‚îî‚îÄ‚îÄ cli.py             # Command-line interface
‚îú‚îÄ‚îÄ web_app/               # Web interface
‚îÇ   ‚îî‚îÄ‚îÄ app.py            # Streamlit application
‚îú‚îÄ‚îÄ data/                  # Data directory
‚îÇ   ‚îî‚îÄ‚îÄ samples/          # Sample images
‚îú‚îÄ‚îÄ models/               # Model storage
‚îú‚îÄ‚îÄ outputs/             # Output images
‚îú‚îÄ‚îÄ config/              # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml      # Main configuration
‚îú‚îÄ‚îÄ tests/               # Test files
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îú‚îÄ‚îÄ .gitignore          # Git ignore rules
‚îî‚îÄ‚îÄ README.md           # This file
```

## Configuration

The application uses YAML configuration files with environment variable overrides. The default configuration is automatically created at `config/config.yaml`:

```yaml
model:
  name: "timbrooks/instruct-pix2pix"
  device: null  # Auto-detect
  num_inference_steps: 20
  guidance_scale: 7.5
  prompt: "colorize this image"

ui:
  port: 8501
  host: "localhost"
  debug: false

logging:
  level: "INFO"
  file: null
```

### Environment Variables

You can override configuration using environment variables:

```bash
export MODEL_NAME="runwayml/stable-diffusion-v1-5"
export UI_PORT=8080
export LOG_LEVEL="DEBUG"
```

## Usage Examples

### Python API

```python
from src.image_colorizer import ImageColorizer

# Initialize colorizer
colorizer = ImageColorizer(model_name="timbrooks/instruct-pix2pix")

# Colorize an image
colorized = colorizer.colorize("input.jpg", prompt="vibrant colors")

# Save result
colorizer.save_image(colorized, "output.jpg")

# Batch processing
images = ["img1.jpg", "img2.jpg", "img3.jpg"]
results = colorizer.batch_colorize(images)
```

### CLI Examples

```bash
# Basic colorization
python src/cli.py colorize photo.jpg colorized_photo.jpg

# Custom settings
python src/cli.py colorize photo.jpg output.jpg \
  --prompt "realistic colors" \
  --steps 30 \
  --guidance 10.0

# Batch processing
python src/cli.py batch photos/ colorized_photos/

# Create sample data
python src/cli.py samples test_data/
```

### Web Interface Features

- **Drag & Drop Upload**: Easy image upload
- **Real-time Preview**: See original and colorized images side-by-side
- **Adjustable Settings**: Modify inference steps, guidance scale, and prompts
- **Download Results**: Save colorized images directly
- **Sample Images**: Built-in sample images for testing

## üîß Advanced Configuration

### Model Selection

The application supports various models:

- `timbrooks/instruct-pix2pix` (default) - Good general-purpose colorization
- `runwayml/stable-diffusion-v1-5` - High-quality results, slower
- Custom models from Hugging Face Hub

### Performance Optimization

1. **GPU Acceleration**: Automatically uses CUDA or MPS when available
2. **Memory Management**: Efficient batch processing for multiple images
3. **Caching**: Models are cached locally after first download

### Custom Prompts

Experiment with different prompts for various colorization styles:

- `"colorize this image"` - Natural colors
- `"vibrant colors"` - More saturated colors
- `"realistic colors"` - Photorealistic results
- `"artistic colors"` - Creative color schemes

## Testing

Run the test suite:

```bash
pytest tests/ -v
```

Run with coverage:

```bash
pytest tests/ --cov=src --cov-report=html
```

## Performance

### Hardware Requirements

- **Minimum**: 8GB RAM, CPU-only processing
- **Recommended**: 16GB RAM, NVIDIA GPU with 8GB+ VRAM
- **Optimal**: 32GB RAM, RTX 4090 or similar

### Processing Times

| Image Size | CPU (Intel i7) | GPU (RTX 3080) |
|------------|----------------|----------------|
| 512x512    | ~2 minutes     | ~15 seconds    |
| 1024x1024  | ~8 minutes     | ~45 seconds    |

## Troubleshooting

### Common Issues

1. **Out of Memory**: Reduce image size or use CPU processing
2. **Model Download Fails**: Check internet connection and Hugging Face access
3. **CUDA Errors**: Ensure PyTorch is installed with CUDA support

### Debug Mode

Enable debug logging:

```bash
export LOG_LEVEL="DEBUG"
python src/cli.py colorize input.jpg output.jpg
```

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and add tests
4. Run the test suite: `pytest tests/`
5. Commit your changes: `git commit -m "Add feature"`
6. Push to the branch: `git push origin feature-name`
7. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- [Hugging Face](https://huggingface.co/) for providing the transformers library and model hub
- [Stable Diffusion](https://stability.ai/) for the underlying diffusion models
- [Streamlit](https://streamlit.io/) for the web interface framework

## References

- [InstructPix2Pix Paper](https://arxiv.org/abs/2211.09800)
- [Stable Diffusion Paper](https://arxiv.org/abs/2112.10752)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)


# AI-Image-Colorization-v2
